{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LITO7\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "\n",
    "# Dependency imports\n",
    "from absl import flags\n",
    "from matplotlib import cm\n",
    "from matplotlib import figure\n",
    "from matplotlib.backends import backend_agg\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfd = tfp.distributions\n",
    "\n",
    "flags.DEFINE_float(\"learning_rate\",\n",
    "                   default=0.01,\n",
    "                   help=\"Initial learning rate.\")\n",
    "flags.DEFINE_integer(\"max_steps\",\n",
    "                     default=1500,\n",
    "                     help=\"Number of training steps to run.\")\n",
    "flags.DEFINE_integer(\"batch_size\",\n",
    "                     default=32,\n",
    "                     help=\"Batch size.\")\n",
    "flags.DEFINE_string(\n",
    "    \"model_dir\",\n",
    "    default='C:\\\\Users\\\\LITO7\\\\Documents\\\\GitHub\\\\BDL\\\\tmp',\n",
    "    help=\"Directory to put the model's fit.\")\n",
    "flags.DEFINE_integer(\"num_examples\",\n",
    "                     default=256,\n",
    "                     help=\"Number of datapoints to generate.\")\n",
    "flags.DEFINE_integer(\"num_monte_carlo\",\n",
    "                     default=50,\n",
    "                     help=\"Monte Carlo samples to visualize weight posterior.\")\n",
    "\n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Warning: deleting old log directory at C:\\Users\\LITO7\\Documents\\GitHub\\BDL\\tmp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LITO7\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\util\\tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  return _inspect.getargspec(target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:   0 Loss: 0.693 Accuracy: 0.531\n",
      "Step: 100 Loss: 0.476 Accuracy: 0.821\n",
      "Step: 200 Loss: 0.351 Accuracy: 0.875\n",
      "Step: 300 Loss: 0.378 Accuracy: 0.894\n",
      "Step: 400 Loss: 0.296 Accuracy: 0.903\n",
      "Step: 500 Loss: 0.315 Accuracy: 0.909\n",
      "Step: 600 Loss: 0.258 Accuracy: 0.913\n",
      "Step: 700 Loss: 0.119 Accuracy: 0.915\n",
      "Step: 800 Loss: 0.220 Accuracy: 0.917\n",
      "Step: 900 Loss: 0.178 Accuracy: 0.919\n",
      "Step: 1000 Loss: 0.131 Accuracy: 0.920\n",
      "Step: 1100 Loss: 0.341 Accuracy: 0.921\n",
      "Step: 1200 Loss: 0.201 Accuracy: 0.922\n",
      "Step: 1300 Loss: 0.240 Accuracy: 0.922\n",
      "Step: 1400 Loss: 0.186 Accuracy: 0.923\n",
      "saved C:\\Users\\LITO7\\Documents\\GitHub\\BDL\\tmp\\weights_inferred.png\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LITO7\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2971: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "def toy_logistic_data(num_examples, input_size=2, weights_prior_stddev=5.0):\n",
    "  \"\"\"Generates synthetic data for binary classification.\n",
    "  Args:\n",
    "    num_examples: The number of samples to generate (scalar Python `int`).\n",
    "    input_size: The input space dimension (scalar Python `int`).\n",
    "    weights_prior_stddev: The prior standard deviation of the weight\n",
    "      vector. (scalar Python `float`).\n",
    "  Returns:\n",
    "    random_weights: Sampled weights as a Numpy `array` of shape\n",
    "      `[input_size]`.\n",
    "    random_bias: Sampled bias as a scalar Python `float`.\n",
    "    design_matrix: Points sampled uniformly from the cube `[-1,\n",
    "       1]^{input_size}`, as a Numpy `array` of shape `(num_examples,\n",
    "       input_size)`.\n",
    "    labels: Labels sampled from the logistic model `p(label=1) =\n",
    "      logistic(dot(features, random_weights) + random_bias)`, as a Numpy\n",
    "      `int32` `array` of shape `(num_examples, 1)`.\n",
    "  \"\"\"\n",
    "  random_weights = weights_prior_stddev * np.random.randn(input_size)\n",
    "  random_bias = np.random.randn()\n",
    "  design_matrix = np.random.rand(num_examples, input_size) * 2 - 1\n",
    "  logits = np.reshape(\n",
    "      np.dot(design_matrix, random_weights) + random_bias,\n",
    "      (-1, 1))\n",
    "  p_labels = 1. / (1 + np.exp(-logits))\n",
    "  labels = np.int32(p_labels > np.random.rand(num_examples, 1))\n",
    "  return random_weights, random_bias, np.float32(design_matrix), labels\n",
    "\n",
    "\n",
    "def visualize_decision(features, labels, true_w_b, candidate_w_bs, fname):\n",
    "  \"\"\"Utility method to visualize decision boundaries in R^2.\n",
    "  Args:\n",
    "    features: Input points, as a Numpy `array` of shape `[num_examples, 2]`.\n",
    "    labels: Numpy `float`-like array of shape `[num_examples, 1]` giving a\n",
    "      label for each point.\n",
    "    true_w_b: A `tuple` `(w, b)` where `w` is a Numpy array of\n",
    "       shape `[2]` and `b` is a scalar `float`, interpreted as a\n",
    "       decision rule of the form `dot(features, w) + b > 0`.\n",
    "    candidate_w_bs: Python `iterable` containing tuples of the same form as\n",
    "       true_w_b.\n",
    "    fname: The filename to save the plot as a PNG image (Python `str`).\n",
    "  \"\"\"\n",
    "  fig = figure.Figure(figsize=(6, 6))\n",
    "  canvas = backend_agg.FigureCanvasAgg(fig)\n",
    "  ax = fig.add_subplot(1, 1, 1)\n",
    "  ax.scatter(features[:, 0], features[:, 1],\n",
    "             c=np.float32(labels[:, 0]),\n",
    "             cmap=cm.get_cmap(\"binary\"),\n",
    "             edgecolors=\"k\")\n",
    "\n",
    "  def plot_weights(w, b, **kwargs):\n",
    "    w1, w2 = w\n",
    "    x1s = np.linspace(-1, 1, 100)\n",
    "    x2s = -(w1  * x1s + b) / w2\n",
    "    ax.plot(x1s, x2s, **kwargs)\n",
    "\n",
    "  for w, b in candidate_w_bs:\n",
    "    plot_weights(w, b,\n",
    "                 alpha=1./np.sqrt(len(candidate_w_bs)),\n",
    "                 lw=1, color=\"blue\")\n",
    "\n",
    "  if true_w_b is not None:\n",
    "    plot_weights(*true_w_b, lw=4,\n",
    "                 color=\"green\", label=\"true separator\")\n",
    "\n",
    "  ax.set_xlim([-1.5, 1.5])\n",
    "  ax.set_ylim([-1.5, 1.5])\n",
    "  ax.legend()\n",
    "\n",
    "  canvas.print_figure(fname, format=\"png\")\n",
    "  print(\"saved {}\".format(fname))\n",
    "\n",
    "\n",
    "def build_input_pipeline(x, y, batch_size):\n",
    "  \"\"\"Build a Dataset iterator for supervised classification.\n",
    "  Args:\n",
    "    x: Numpy `array` of features, indexed by the first dimension.\n",
    "    y: Numpy `array` of labels, with the same first dimension as `x`.\n",
    "    batch_size: Number of elements in each training batch.\n",
    "  Returns:\n",
    "    batch_features: `Tensor` feed  features, of shape\n",
    "      `[batch_size] + x.shape[1:]`.\n",
    "    batch_labels: `Tensor` feed of labels, of shape\n",
    "      `[batch_size] + y.shape[1:]`.\n",
    "  \"\"\"\n",
    "  training_dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "  training_batches = training_dataset.repeat().batch(batch_size)\n",
    "  training_iterator = training_batches.make_one_shot_iterator()\n",
    "  batch_features, batch_labels = training_iterator.get_next()\n",
    "  return batch_features, batch_labels\n",
    "\n",
    "\n",
    "def main(argv):\n",
    "  del argv  # unused\n",
    "  if tf.gfile.Exists(FLAGS.model_dir):\n",
    "    tf.logging.warning(\n",
    "        \"Warning: deleting old log directory at {}\".format(FLAGS.model_dir))\n",
    "    tf.gfile.DeleteRecursively(FLAGS.model_dir)\n",
    "  tf.gfile.MakeDirs(FLAGS.model_dir)\n",
    "\n",
    "  # Generate (and visualize) a toy classification dataset.\n",
    "  w_true, b_true, x, y = toy_logistic_data(FLAGS.num_examples, 2)\n",
    "  features, labels = build_input_pipeline(x, y, FLAGS.batch_size)\n",
    "\n",
    "  # Define a logistic regression model as a Bernoulli distribution\n",
    "  # parameterized by logits from a single linear layer. We use the Flipout\n",
    "  # Monte Carlo estimator for the layer: this enables lower variance\n",
    "  # stochastic gradients than naive reparameterization.\n",
    "  with tf.name_scope(\"logistic_regression\", values=[features]):\n",
    "    layer = tfp.layers.DenseFlipout(\n",
    "        units=1,\n",
    "        activation=None,\n",
    "        kernel_posterior_fn=tfp.layers.default_mean_field_normal_fn(),\n",
    "        bias_posterior_fn=tfp.layers.default_mean_field_normal_fn())\n",
    "    logits = layer(features)\n",
    "    labels_distribution = tfd.Bernoulli(logits=logits)\n",
    "\n",
    "  # Compute the -ELBO as the loss, averaged over the batch size.\n",
    "  neg_log_likelihood = -tf.reduce_mean(labels_distribution.log_prob(labels))\n",
    "  kl = sum(layer.losses) / FLAGS.num_examples\n",
    "  elbo_loss = neg_log_likelihood + kl\n",
    "\n",
    "  # Build metrics for evaluation. Predictions are formed from a single forward\n",
    "  # pass of the probabilistic layers. They are cheap but noisy predictions.\n",
    "  predictions = tf.cast(logits > 0, dtype=tf.int32)\n",
    "  accuracy, accuracy_update_op = tf.metrics.accuracy(\n",
    "      labels=labels, predictions=predictions)\n",
    "\n",
    "  with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)\n",
    "    train_op = optimizer.minimize(elbo_loss)\n",
    "\n",
    "  init_op = tf.group(tf.global_variables_initializer(),\n",
    "                     tf.local_variables_initializer())\n",
    "\n",
    "  with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "\n",
    "    # Fit the model to data.\n",
    "    for step in range(FLAGS.max_steps):\n",
    "      _ = sess.run([train_op, accuracy_update_op])\n",
    "      if step % 100 == 0:\n",
    "        loss_value, accuracy_value = sess.run([elbo_loss, accuracy])\n",
    "        print(\"Step: {:>3d} Loss: {:.3f} Accuracy: {:.3f}\".format(\n",
    "            step, loss_value, accuracy_value))\n",
    "\n",
    "    # Visualize some draws from the weights posterior.\n",
    "    w_draw = layer.kernel_posterior.sample()\n",
    "    b_draw = layer.bias_posterior.sample()\n",
    "    candidate_w_bs = []\n",
    "    for _ in range(FLAGS.num_monte_carlo):\n",
    "      w, b = sess.run((w_draw, b_draw))\n",
    "      candidate_w_bs.append((w, b))\n",
    "    visualize_decision(x, y, (w_true, b_true),\n",
    "                       candidate_w_bs,\n",
    "                       fname=os.path.join(FLAGS.model_dir,\n",
    "                                          \"weights_inferred.png\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  tf.app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
